---
title: "Упражнение 2"
author: "Нестерова А.И."
date: "23 02 2020"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Математическое моделирование

### Оценка точности модели с дискретной зависимой переменной (Y)

## Упражнение 2

1. Построить модели на данных примера 3 с параметрами распределений, соответствующими своему варианту. На графиках сетку с истинной разделяющей границей рисовать не нужно. Определить, какой из методов срабатывает на этих данных лучше, и почему.    

2. По матрице неточностей той модели, которая оказалась лучше по $Acc$, рассчитать характеристики качества и ошибки из лекции: $TPR$, $SPC$, $PPV$, $NPV$, $FNR$, $FPR$, $FDR$, $MCC$.   

3. Выполненные задачи 1-2 разместить в одном отчёте в репозитории на github.com, выслать ссылку на него на почту преподавателя. В репозитории должны лежать:

- файл README.md с кратким описанием содержимого репозитория;    

- скрипт генерации отчёта: файл .Rmd в кодировке UTF-8;   

- графики, сгенерированные в задании. 

### Вариант - 13

- класс $Y=0$: $X \sim N((5, 12), \begin{pmatrix}
  8.1^2 & 0 \\
  0 & 7.6^2 \end{pmatrix})$   

- класс $Y=1$: $X \sim N((7, 22), \begin{pmatrix}
  6.9^2 & 0 \\
  0 & 15.4^2 \end{pmatrix})$ 

```{r, message = FALSE, warning = F}
library('knitr')        # пакет для генерации отчёта
library('class')        # функция knn()
library('e1071')        # функция naiveBayes()
library('MASS')         # функция mvrnorm()

my.seed <- 12345
n <- 100               # наблюдений всего
train.percent <- 0.85  # доля обучающей выборки

# x-ы -- двумерные нормальные случайные величины

set.seed(my.seed)

class.0 <- mvrnorm(45, mu = c(5, 12), 
                   Sigma = matrix(c(8.1^2, 0, 0, 7.6^2), 2, 2, 
                                  byrow = T))

set.seed(my.seed + 1)
class.1 <- mvrnorm(55, mu = c(7, 22), 
                   Sigma = matrix(c(6.9^2, 0, 0, 15.4^2), 2, 2, 
                                 byrow = T))

# записываем x-ы в единые векторы (объединяем классы 0 и 1)
x1 <- c(class.0[, 1], class.1[, 1])
x2 <- c(class.0[, 2], class.1[, 2])

# фактические классы Y
y <- c(rep(0, nrow(class.0)), rep(1, nrow(class.1)))

# классы для наблюдений сетки
rules <- function(x1, x2){
  ifelse(x2 < 1.6*x1 + 19, 0, 1)
  }

# Отбираем наблюдения в обучающую выборку --------------------------------------

set.seed(my.seed)
inTrain <- sample(seq_along(x1), train.percent*n)
x1.train <- x1[inTrain]
x2.train <- x2[inTrain]
x1.test <- x1[-inTrain]
x2.test <- x2[-inTrain]

# используем истинные правила, чтобы присвоить фактические классы
y.train <- y[inTrain]
y.test <- y[-inTrain]

# фрейм с обучающей выборкой
df.train.1 <- data.frame(x1 = x1.train, x2 = x2.train, y = y.train)
# фрейм с тестовой выборкой
df.test.1 <- data.frame(x1 = x1.test, x2 = x2.test)
```

Нарисуем обучающую выборку на графике. Сеткой точек показаны области классов, соответствующие истинным дискриминирующим правилам.

```{r, fig.height = 5, fig.width = 5}
# Рисуем обучающую выборку графике ---------------------------------------------

# цвета для графиков
cls <- c('blue', 'orange')
cls.t <- c(rgb(0, 0, 1, alpha = 0.5), rgb(1,0.5,0, alpha = 0.5))

# график истинных классов
plot(df.train.1$x1, df.train.1$x2, 
     pch = 21, bg = cls.t[df.train.1[, 'y'] + 1], 
     col = cls[df.train.1[, 'y'] + 1],
     xlab = 'X1', ylab = 'Y1',
     main = 'Обучающая выборка, факт')


```

Обучим модель **наивного байесовского классификатора** и оценим её точность (верность) на обучающей выборке. Поскольку объясняющие переменные для классов сгенерированы как двумерные нормальные распределения и сами классы не перекрываются, следует ожидать, что эта модель окажется точной.   

```{r, fig.height = 5, fig.width = 5}
# Байесовский классификатор ----------------------------------------------------
#  наивный байес: непрерывные объясняющие переменные

# строим модель
nb <- naiveBayes(y ~ ., data = df.train.1)
# получаем модельные значения на обучающей выборке как классы
y.nb.train <- ifelse(predict(nb, df.train.1[, -3], 
                             type = "raw")[, 2] > 0.5, 1, 0)

# точки наблюдений, предсказанных по модели
plot(df.train.1$x1, df.train.1$x2, 
     pch = 21, bg = cls.t[y.nb.train + 1],
     col = cls[y.nb.train + 1], 
     xlab = 'X1', ylab = 'Y1',
     main = 'Обучающая выборка, модель naiveBayes')

# матрица неточностей на обучающей выборке
tbl1 <- table(y.train, y.nb.train)
tbl1

# точность, или верность (Accuracy)
Acc1 <- sum(diag(tbl1)) / sum(tbl1)
Acc1
```

Сделаем прогноз классов Y на тестовую выборку и оценим точность модели.   

```{r, fig.height = 5, fig.width = 5}

# прогноз на тестовую выборку
y.nb.test <- ifelse(predict(nb, df.test.1, type = "raw")[, 2] > 0.5, 1, 0)

# матрица неточностей на тестовой выборке
tbl1 <- table(y.test, y.nb.test)
tbl1

# точность, или верность (Accuracy)
Acc1 <- sum(diag(tbl1)) / sum(tbl1)
Acc1

```

Построим модель **kNN**. С этими данными у метода не должно возникнуть проблем, так как классы не смешиваются.      


```{r, fig.height = 5, fig.width = 5}

# Метод kNN --------------------------------------------------------------------
#  k = 3

# строим модель и делаем прогноз
y.knn.train <- knn(train = scale(df.train.1[, -3]), 
                   test = scale(df.train.1[, -3]),
                   cl = df.train.1$y, k = 3)


# точки наблюдений, предсказанных по модели
plot(df.train.1$x1, df.train.1$x2, 
     pch = 21, bg = cls.t[as.numeric(y.knn.train)], 
     col = cls.t[as.numeric(y.knn.train)],
     xlab = 'X1', ylab = 'Y1',
     main = 'Обучающая выборка, модель kNN')

# матрица неточностей на обучающей выборке
tbl2 <- table(y.train, y.knn.train)
tbl2

# точность (Accuracy)
Acc2 <- sum(diag(tbl2)) / sum(tbl2)
Acc2
```

Сделаем прогноз классов Y на тестовую выборку и оценим точность модели. 

```{r, fig.height = 5, fig.width = 5}

# прогноз на тестовую выборку
y.knn.test <- knn(train = scale(df.train.1[, -3]), 
                  test = scale(df.test.1[, -3]),
                  cl = df.train.1$y, k = 3)

# матрица неточностей на тестовой выборке
tbl2 <- table(y.test, y.knn.test)
tbl2

# точность (Accuracy)
Acc2 <- sum(diag(tbl2)) / sum(tbl2)
Acc2

```

Так как значения Acc по тестовой выборке оказались одинаковыми для обеих моделей, рассчитаем для них характеристики качества.

```{r characteristics}
TPR <- c(round(tbl1[2,2]/sum(tbl1[2,]),3), round(tbl2[2,2]/sum(tbl2[2,]),3)) # чувствительность
SPC <- c(round(tbl1[1,1]/sum(tbl1[,1]),3), round(tbl2[1,1]/sum(tbl2[,1]),3)) # специфичность
PPV <- c(round(tbl1[2,2]/sum(tbl1[,2]),3), round(tbl2[2,2]/sum(tbl2[,2]),3)) # ценность положительного прогноза
NPV <- c(round(tbl1[1,1]/sum(tbl1[,1]),3), round(tbl2[1,1]/sum(tbl2[,1]),3)) # ценность отрицательного прогноза
FNR <- 1-TPR # доля положительных исходов
FPR <- 1-SPC # доля ложных срабатываний
FDR <- 1-PPV # доля ложного обнаружения
MCC <- c(round((tbl1[1,1]*tbl1[2,2]-tbl1[1,2]*tbl1[2,1])/sqrt(sum(tbl1[,2])*sum(tbl1[2,])*sum(tbl1[1,])*sum(tbl1[,1])),3), round((tbl2[1,1]*tbl2[2,2]-tbl2[1,2]*tbl2[2,1])/sqrt(sum(tbl2[,2])*sum(tbl2[2,])*sum(tbl2[1,])*sum(tbl2[,1])),3)) # корреляция Мэтьюса 

ch.fr <- rbind(TPR, SPC, PPV, NPV, FNR, FPR, FDR, MCC)
colnames(ch.fr)<-c('Модель 1', 'Модель 2')
kable(ch.fr)
```

Модель 1 (**наивного байесовского классификатора**) признается лучшей, по критериям $TPR$ (чувствительность), $SPS$ (специфичность), $FNR$ (доля ложнноотрицательных исходов), $FPR$ (доля ложных срабатываний). Модель 2 (**kNN**) признаётся лучшей по критериям $PPV$ (ценность положительного прогноза), $FDR$ (доля ложного обнаружения) и $MCC$ (корреляция Мэтьюса).